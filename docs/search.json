[
  {
    "objectID": "old/Posts/LICENSE.html",
    "href": "old/Posts/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nSoftware License Agreement (BSD License)\nCopyright 2013 Yahoo! Inc. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n* Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution.\n\n* Neither the name of the Yahoo! Inc. nor the\n  names of its contributors may be used to endorse or promote products\n  derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL YAHOO! INC. BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a postdoctoral researcher in Data Intensive Astronomy at the University of Macquarie."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education"
  },
  {
    "objectID": "about.html#research",
    "href": "about.html#research",
    "title": "About",
    "section": "Research",
    "text": "Research"
  },
  {
    "objectID": "posts/sparse-bayesian-regression/index.html",
    "href": "posts/sparse-bayesian-regression/index.html",
    "title": "Sparse Bayesian Regression",
    "section": "",
    "text": "I’ve been reading the excellent blog by Michael Betancourt, who’s an applied statistician (and ex-physicist), for a while. He works on Hamiltonian Monte Carlo and its applications to statistical modelling problems, primarily through the language Stan. Whilst reading through his website, I thought it’d be a useful way to get more acquainted with PyMC3 (a python statistical modelling package which is similar to Stan) by translating some of his case studies to Python. The following is my attempt at performing “Sparse Bayesian Regression”, which is very important when you have a large number of variables and you’re not sure which ones are important. You can read Michael’s original blog, on which the following is based, here."
  },
  {
    "objectID": "posts/sparse-bayesian-regression/index.html#sparse-bayesian-regression",
    "href": "posts/sparse-bayesian-regression/index.html#sparse-bayesian-regression",
    "title": "Sparse Bayesian Regression",
    "section": "Sparse Bayesian Regression",
    "text": "Sparse Bayesian Regression\nLinear regression problems involve trying to predict the value of some variable, \\(y\\), based on one or a number of input variables, \\(X\\). \\(X\\) may be a single vector of \\(N\\) numbers or an \\(M\\times N\\) matrix, where we have \\(M\\) variables with \\(N\\) observations of each one.\nPut another way, let’s say we want to make a model to predict the life expectancy of people in the UK (our \\(y\\) variable). Our inputs could be things like income, postcode and amount of exercise each person does a week. If we had 100 people’s data in our model, our \\(X\\) would then be a matrix of shape \\(3\\times 100\\). We might then (very simply) model the relationship between our input variables and our output variable as a linear one, such that we could write\n\\[\n\\begin{align}\n    y &\\sim m \\cdot X +c + \\epsilon,\n    \\\\\n    \\epsilon & \\sim\\mathrm{Normal}(0, \\sigma^2)\n\\end{align}\n\\]\nwith \\(\\epsilon\\) some “intrinsic scatter” in the relation and \\(m\\) and \\(c\\) some unknown numbers which we could find (by Maximum Likelihood methods or by sampling from the posterior using something like PyMC3 or emcee.\nThis is fine when we know exactly what our input data (\\(X\\)) should going to be. But what about when we have lots of things which might play a role in our model? Presumably we have more data than just the three variables I mentioned before. Why do we exclude things like “number of pairs of socks owned” or “distance from each person’s house to the nearest cinema”? Whilst it’s (very) unlikely that these parameters would have a large effect on a person’s life expectancy, wouldn’t it be nice for the data to tell us that itself?\nThis is where sparse methods come in. Sparsity is the assumption that although we may have lots of variables, we think that only a few should contribute to our final model. In frequentist statistics, methods like LASSO regression try and penalise large values of the gradient terms in our model (the \\(m\\)s from before) such that only a few of them (the most important ones) end up in our final model, and most of the others are forced to be very small. This process is known as regularisation. I hadn’t realised that transferring across these techniques to Bayesian statistics wasn’t completely straightforward, but Micheal’s case study on the topic summarises the issues nicely.\nEssentially, applying a simple penalty to the likelihood function using a prior of the form\n\\[\n\\mathcal{L}\\sim\\sum_{i=0}^{N}\\lambda_{i}m_{i}\n\\]\nas in LASSO will fail because this was designed for frequentist point estimates. In Bayesian methods, we care about the whole distribution of values each \\(m_i\\) can take- and so values of the posterior below the regularisation scale \\(\\lambda_i\\) will be driven to zero but values above it will be left alone. This tug-of-war on each end of our posterior isn’t what we want at all, and leads to biased inference and to wider distributions than necessary.\nSo what’s the solution? We need a prior distribution which acts on the whole posterior distribution of each \\(m_i\\), either driving it to zero or leaving it alone. This turns out to be an active area of research in statistics, and Carvalho, Polson, and Scott (2009) and Piironen and Vehtari (2017a) have come up with the “Horseshoe” and “Finnish Horseshoe” priors to come to our rescue."
  },
  {
    "objectID": "posts/sparse-bayesian-regression/index.html#a-concrete-example",
    "href": "posts/sparse-bayesian-regression/index.html#a-concrete-example",
    "title": "Sparse Bayesian Regression",
    "section": "A concrete example",
    "text": "A concrete example\nBefore we look at those, let’s set up an example and see why our normal way of solving it fails. We’ll say we have 200 different variables (\\(M\\)) and 100 different observations of each (\\(N\\)). Note that having \\(M &gt; N\\) can be tricky, since the model is “non-identifiable”- there could be more than set of \\(m\\) and \\(c\\) values which give the same \\(y\\). It’s only the fact that we know (or assume) that lots of our \\(M\\) variables can be ignored which allows us to come up with useful results here.\nWe’ll make some fake data which follows a linear relationship, then try and infer the values of the parameters using PyMC3. Most of the gradient terms in the model will be around 0, but a small number won’t be:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nM = 200  # Number of possible variables\nN = 100  # Number of observations per variable\n\n# These are the true values of the parameters\n# We'll try and infer these using PyMC3\nc = 3.0  # Constant offset in the linear relation\nsigma = 1.0  # Intrinsic scatter in the model\n\n# Now go through and make 200 different true values of the slope\n# A few will be bigger/smaller than 0, most won't be\n\nprob_slope_is_meaninful = 0.05\nm = np.empty(M)\nfor i in range(M):\n    if np.random.binomial(1, 0.05):  # random chance for slope to be 'meaningful'\n        # Now select a positive or negative gradient with p=0.5\n        if np.random.binomial(1, 0.5):\n            beta[i] = np.random.randn() + 10.0\n        else:\n            beta[i] = np.random.randn() - 10.0\n    else:\n        beta[i] = np.random.randn() * 0.25\n\n\n# Our inputs, X,  and observed data, y\nX = np.random.randn(M, N)\n# Python3 dot product plus offset plus intrinsic scatter\nY = X.T @ m + c + np.random.randn(N) * sigma\n\n\nNow, if we know that our gradient values are going to be somewhere between -10 and 10 (by, say, plotting the data and having a look at it), we might think that the following simple model would work:\n\n\nCode\n    with pm.Model() as simple_model:\n        #What if we try and use a wide prior here?\n        gradient=pm.Normal('m', mu=0.0, sd=10.0, shape=M)\n        offset=pm.Normal('c', mu=0.0, sd=2.0)\n        sigma=pm.HalfNormal('sig', sd=2.0)\n\n        y_inference=pm.Normal('y', mu=pm.math.dot(X.T, gradient)+offset, sd=sigma, observed=Y)\n\n        trace_wide=pm.sample(2000)\n\n\nIdeally, we’d hope that this would have the flexibility we’d like- gradient terms around 0 to stay at 0 and the few large values to settle on their correct values. However, when we run things our posterior looks like this:\n\n\n\nPosterior with a wide prior\n\n\nHere I’ve plotted the posterior distribution of the gradient for each of the 200 variables we have. In black are the true values which we’re trying to recover. The median of the posterior is shown in grey for each variable, and we’re hoping that those bars closely follow the black outlines. The shaded regions show the 68%, 90% and 95% confidence regions for each parameter, and so ideally we want those to be as small as possible. Since we can see they’re very large, this plot is telling us that this model has failed miserably- you really can’t say much about any one of these gradient values with any confidence. I also tried a Laplace prior (in the spirit of “Bayesian LASSO”- from e.g Park et al. 2008) instead of a Normal one, which showed an improvement but still wasn’t completely satisfactory.\nCan the Finnish Horseshoe help? It’s quite a tricky hierarchical model, and following Micheal’s blog post led me to changing some of the default HMC NUTS parameters. You can see that we’re also using a “non-centered” model which is often easier to sample from:\n\n\nCode\n    #Guess as to number of 'meaningful' gradients we have- order of magnitude is okay!\n    mo=10\n    #Other constants for the model\n    slab_scale = 3\n    slab_scale_squared=slab_scale*slab_scale\n    slab_degrees_of_freedom=25\n    half_slab_df=slab_degrees_of_freedom*0.5\n\n    with pm.Model() as finnish_horseshoe_prior:\n\n        tau0 = (m0 / (M - m0)) * (sigma / np.sqrt(1.0 * N))\n\n        #Non-centered distributions- loc=0, width=1 then shift/stretch afterwards\n        gradient_tilde = pm.Normal('gradient_tilde', mu=0, sd=1, shape=M, testval=0.1)\n        lamda = pm.HalfCauchy('lamda', beta=1, shape=M, testval=1.0)\n        tau_tilde = pm.HalfCauchy('tau_tilde', beta=1, testval=0.1)\n        c2_tilde = pm.InverseGamma('c2_tilde', alpha=half_slab_df, beta=half_slab_df, testval=0.5)\n\n        #Do the shifting/stretching\n        tau=pm.Deterministic('tau', tau_tilde*tau0)\n        c2=pm.Deterministic('c2',slab_scale_squared*c2_tilde)\n        lamda_tilde =pm.Deterministic('lamda_tilde', pm.math.sqrt((c2 * pm.math.sqr(lamda) / (c2 + pm.math.sqr(tau) * pm.math.sqr(lamda)) ))) \n\n        #Linear model variables as before\n        gradient = pm.Deterministic('m', tau * lamda_tilde * gradient_tilde)\n        constant=pm.Normal('c', mu=0.0, sd=2.0, testval=1.0)\n        mu=pm.Deterministic('mu', pm.math.dot(X.T, gradient)+constant)\n        sigma=pm.Normal('sig', mu=0.0, sd=2.0, testval=1.0)\n\n        y_inference=pm.Normal('y', mu=mu, sd=sigma, observed=Y)\n\n        #Nuts with changed defaults\n        trace=pm.sample(2000, init='advi+adapt_diag', nuts_kwargs={'target_accept':0.99, 'max_treedepth':15})\n\n\nLetting this thing sample for a while (a long while on my laptop…) gives us the following posterior:\n\n\n\nPosterior with a horseshoe prior\n\n\nSuccess! The Finnish Horseshoe prior has shrunk most of our gradient terms to zero but the wide Half-Cauchy tails of the \\(\\lambda\\) parameter allows some of the terms to escape and take on values above/below 0. This is especially evident in the residuals plot, where I’ve subtracted the true values of each gradient term. As you can see, the values are now all scattered around 0 and the confidence regions are about an order of magnitude smaller than in our first attempt:\n\n\n\nResiduals of hierarchical model\n\n\nYou can find all the PyMC3 code to run this example here"
  },
  {
    "objectID": "posts/hierarchical-model-world-cup/index.html",
    "href": "posts/hierarchical-model-world-cup/index.html",
    "title": "World Cup predictions from a hierarchical Bayesian model",
    "section": "",
    "text": "I wrote a simple model to “predict” the outcome of the 2018 FIFA world cup. You can read about it on my Medium page, here"
  },
  {
    "objectID": "posts/my-vscode-settings/index.html",
    "href": "posts/my-vscode-settings/index.html",
    "title": "My VS code settings",
    "section": "",
    "text": "I’ve been playing around with my VS code settings to make its integration with the IPython terminal a little better. I’ve never really liked Jupyter notebooks, so all of my coding involves me editing in a python script and sending code to an IPython terminal, or sometimes working in a Quarto notebook.\nWhilst working with R in VSCode, I’ve really liked being able to send individual lines to the radian R console using cmd+enter and running the entire script using cmd+shift+s. It was actually a bit tricky to get the same behavious in VSCode with IPython: I’m documenting it here for posterity!\nThe following comes from a comment on this very helpful stackoverflow question and this Github gist. Using this Macro extension for VScode, I added the following to my settings.json:\n    \"macros.list\": {\n        \"runInIPythonTerminal\": [\n            {\"command\": \"workbench.action.terminal.sendSequence\",\"args\": { \"text\": \"%run \\\"${file}\\\"\"}},\n            {\"command\": \"$delay\",\"args\": {\"delay\": 100}},\n            {\"command\": \"workbench.action.terminal.sendSequence\",\"args\": { \"text\": \"\\r\" }},\n            ],\n    },\nwhich makes a new macro to send %run {current_python_filename} to the terminal. I then map this to a new keybinding:\n\n  {\n    \"key\": \"shift+cmd+s\",\n    \"command\": \"macros.runInIPythonTerminal\",\n    \"when\": \"editorTextFocus && !findInputFocussed && !jupyter.ownsSelection && !notebookEditorFocused && !replaceInputFocussed && editorLangId == 'python'\"\n  }\nwhere the “when” section tells VSCode to only run this command under certain circumstances (i.e. when I’m in the text editor and working on a python file). I’ve also added this to run a single line:\n\n  {\n    \"key\": \"cmd+enter\",\n    \"command\": \"python.execSelectionInTerminal\",\n    \"when\": \"editorTextFocus && !findInputFocussed && !jupyter.ownsSelection && !notebookEditorFocused && !replaceInputFocussed && editorLangId == 'python'\"\n  },\nand unmapped the default shift+enter.\nNote that this isn’t perfect- it doesn’t work for sending multiple lines to IPython at once. For example, pressing cmd+enter on first of the following lines\n# | eval : false\n\n# fmt: off\ndata = dict(\n    a=1, \n    b=2, \n    c=3\n    )\nshould be smart enough to send the entire snippet to IPython- it works very nicely in R with theradian shell- but instead it will only send data = dict( and leave you hanging. You’ll need to highlight the entire three lines, and then press cmd+enter for it to work.\nIt looks like there is a feature request open on Github, so perhaps it might be in the works at some point."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Integral Field Spectroscopy (IFS) is a revolutionary technique for understanding what’s going on within galaxies, combining the spatial resolution of taking an image with the information content of spectroscopy. The now-completed SAMI galaxy survey and the forthcoming Hector Galaxy Survey (for which I’m leading the target selection) use integral-field spectroscopy to study thousands of nearby galaxies, building up a census of our cosmic neighbours. I’m particularly interested in the chemical properties of the stars in these galaxies, and studying how a galaxy’s cosmic history can influence its present-day properties.\n\n\n\nIonised gas kinematic maps I’ve made for the K-CLASH galaxy survey"
  },
  {
    "objectID": "research.html#integral-field-spectroscopy",
    "href": "research.html#integral-field-spectroscopy",
    "title": "Research",
    "section": "",
    "text": "Integral Field Spectroscopy (IFS) is a revolutionary technique for understanding what’s going on within galaxies, combining the spatial resolution of taking an image with the information content of spectroscopy. The now-completed SAMI galaxy survey and the forthcoming Hector Galaxy Survey (for which I’m leading the target selection) use integral-field spectroscopy to study thousands of nearby galaxies, building up a census of our cosmic neighbours. I’m particularly interested in the chemical properties of the stars in these galaxies, and studying how a galaxy’s cosmic history can influence its present-day properties.\n\n\n\nIonised gas kinematic maps I’ve made for the K-CLASH galaxy survey"
  },
  {
    "objectID": "research.html#the-stellar-initial-mass-function",
    "href": "research.html#the-stellar-initial-mass-function",
    "title": "Research",
    "section": "The Stellar Initial Mass function",
    "text": "The Stellar Initial Mass function\nThe Initial Mass Function (IMF) describes the proportion of massive stars in a galaxy to those which are lighter than our sun. It’s key for understanding a galaxy’s lifecycle, and gives important clues to how the galaxy formed in the first place. During my PhD, I tried to measure the IMF within nearby galaxies by looking for the “fingerprints” of low-mass stars in the spectra of their integrated light. I was recently a co-investigator on an accepted JWST proposal to try and do this on some galaxies much, much further away, so watch this space!"
  },
  {
    "objectID": "research.html#statistics-and-software",
    "href": "research.html#statistics-and-software",
    "title": "Research",
    "section": "Statistics and Software",
    "text": "Statistics and Software\nI’m interested in learning new statistical techniques to make better sense of the large amounts of data astronomers produce. I also write open source software (mainly in python, but also in R) and experiment with machine learning tools and deep neural networks. I have developed a package to fit galaxy spectra with population models (PyStaff), written a pure python/numpy implementation of a deep neural network to classify hand-written numbers (MNIST) and collaborated on a project to create plausibly-sounding fake scientific titles using natural language processing tools."
  },
  {
    "objectID": "research.html#observing",
    "href": "research.html#observing",
    "title": "Research",
    "section": "Observing",
    "text": "Observing\nI’ve been lucky enough to travel to some amazing places during my career, with over 75 nights observing experience across the 3.9 metre Anglo-Australian Telescope, the 200-inch telescope at Palomar Observatory, California and the Very Large Telescope in the Atacama Desert, Chile."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam Vaughan",
    "section": "",
    "text": "I’m currently a Data Scientist at Betr. My work involves building deep neural recommender systems. I have previously worked as an Astro3D Postdoctoral Research Associate in Data Intensive Astronomy at Macquarie University in Sydney where I worked on studying the formation and evolution of galaxies. I was previously a postdoc at the University of Sydney, and I received my PhD from the University of Oxford in 2019.\nYou can find a list of my publications here and a recent CV here."
  },
  {
    "objectID": "hector.html",
    "href": "hector.html",
    "title": "The Hector Galaxy Survey",
    "section": "",
    "text": "I’ve been very involed in the Hector Galaxy Survey, a $7m dollar Australian Research Council funded project to meausure the properties of 15,000 galaxies in the night sky.\nThe code I’ve written has helped the survey create catalogues of targets to study, group these big catalogues into individual “tiles”, and turn these tiles into the correct files the astronomers at the telescope need to actually carry out our observations.\nYou can find the documentation I’ve written about these steps below:"
  },
  {
    "objectID": "hector.html#general-workflow-tips",
    "href": "hector.html#general-workflow-tips",
    "title": "The Hector Galaxy Survey",
    "section": "General Workflow Tips",
    "text": "General Workflow Tips\nThe workflow for each step is similar: they all use the woorkflow management tool snakemake. Their directory structures are also follow the same basic structure:\nproject\n└───docs\n└───config\n└───resources\n└───results\n└───workflow\n│   │   Snakefile\n│   └───scripts\nThe docs folder contains the documentation I’ve written for these pipelines. They’re all in markdown files (.md), and are turned into the websites above using the tool MkDocs.\nresources contains input files for this pipeline, i.e. files which the pipeline needs to run but will not change. Files which are created by the pipeline are saved in results. Note that the input to one pipeline is likely to be the output from another! There can also be sub-directories in each of these folders to keep things tidy.\nParameters which are needed during a pipeline run are stored in a config file in the config folder. These config files are always in the YAML format (.yaml, which rhymes with ‘camel’). This is a human-readable format for passing around small pieces of data like numbers and strings. They should be well commented and hopefully fairly easy to understand!\nThe workflow folder contains a folder named scripts, which stores all of the python scripts needed to carry out individual tasks (e.g. selecting galaxies from a catalogue, removing foreground stars, running the distortion correction code, etc). The Snakefile is a series of rules which lays out how each of these scripts work together. The tool snakemake then follows these steps to work out which python scripts it needs to run and in what order.\nAn example snakemake command would be something like:\nsnakemake --cores 1 --config-file config/your_config_file.yaml -- results/path/to/output/file\nThis would tell snakemake to make the file results/path/to/output/file using a single thread (--cores 1) and using the parameters contained in config/your_config_file.yaml.\nA very handy set of options are -npr. If you add these flags at the beginning of the above command, snakemake will print out all the steps it is about to take without actually running them."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "A Macquarie University Machine Learning masterclass”\n\n\n\n\n\n\nmachine-learning\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nMy VS code settings\n\n\n\n\n\n\ncoding\n\n\ntools\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards and data logging with a Raspberry Pi\n\n\n\n\n\n\nraspberry-pi\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Bayesian Regression\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nAug 26, 2018\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Cup predictions from a hierarchical Bayesian model\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nJun 30, 2018\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nA galaxy lacking dark matter?\n\n\n\n\n\n\ngalaxies\n\n\n\n\n\n\n\n\n\nApr 9, 2018\n\n\nSam Vaughan\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks in Python\n\n\n\n\n\n\nstats\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 18, 2018\n\n\nSam Vaughan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html",
    "href": "posts/raspberry-pi-data-logging/index.html",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "",
    "text": "I recently brought a couple of atmospheric sensors for my Raspberry Pi to measure temperature and air pressure. This is a quick post about how I’ve set everything up to collect the data, save it to an Influx database and display it with a Grafana dashboard.\n\n\nI’m using a Raspberry Pi Zero and the PiicoDev precision temperature and pressure sensors, purchased from Core Electronics. They were incredibly easy to set up- just plug and play into a PiicoDev adaptor attached to the Pi’s header pins. The Pi itself was a standard configuration, just Raspberry Pi OS (5.15.84+ “Bullseye”).\nNext, initialise the I2C interface in the Raspberry Pi configuration menu or by using\n\n\nCode\nsudo raspi-config nonint do_i2c 0\n\n\nat the command line. Note that the zero means “on” here! We next need the piicodev python package from pip:\n\n\nCode\npip install piicodev\n\n\nUsing the piicodev package, we can then grab values from these sensors in the following way:\n\n\nCode\n# Import the packages\nfrom PiicoDev_TMP117 import PiicoDev_TMP117 # temperature sensos\nfrom PiicoDev_MS5637 import PiicoDev_MS5637 # pressure sensor\n\n# initialise the sensors\ntempSensor = PiicoDev_TMP117()\npressure = PiicoDev_MS5637()\n\n# read the values\ntempC = tempSensor.readTempC()\npress_hPa = pressure.read_pressure()\n\n\nSo if you want to just show those measurements on the commandline, you can just wrap this in a while loop and add some print statements. I wanted to have things running in the background and the results being saved in a nice format, however, so we also need a database to write to and a way to plot the results.\n\n\n\nWhilst we could write things to a csv file, having a proper database connection has a number of benefits (such as scaling better if/when you have multiple sensors in different locations, and interfacing very nicely with the dashboard service we’ll be using). InfluxDB is an open-source platform built for working with time-series data, so it’s a sensible choice.\nFirst, we install it on our Pi. Which version? The latest online is Version 2.7, but that’s only available for 64-bit architectures, which my Pi Zero sadly isn’t. As of April 2023, version 1.6.7 is available in the standard Raspbian channels, but I’m using version 1.8.1 here.\nTo get that, we need to add the Influx repository\n\n\nCode\ncurl https://repos.influxdata.com/influxdata-archive.key | gpg --dearmor | sudo tee /usr/share/keyrings/influxdb-archive-keyring.gpg &gt;/dev/null\n\n\nand then add that to our source list. Note that $(lsb_release -cs) gets our version of Raspberry Pi OS, which in my case is “Bullseye”.\n\n\nCode\necho \"deb [signed-by=/usr/share/keyrings/influxdb-archive-keyring.gpg] https://repos.influxdata.com/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n\n\nWe can then just do:\n\n\nCode\nsudo apt update && sudo apt install influxdb\n\n\nIf everything goes well, that should all finish without any errors. We then need to start the influx service running, and make sure that it starts whenever our Pi boots up.\n\n\nCode\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n\nYou should now be able to run influx at the command line, which changes to the influx prompt. We need to create a database and make a new user, which we can do with the following commands. Here the new user will be called grafana, with grafana as the password.\n\n\nCode\ncreate database home_measurements\nuse home_measurements\n\ncreate user grafana with password 'grafana' with all privileges\ngrant all privileges on home to grafana\n\n\n\n\n\nWe can write our measurments to our database with a short python script. Firstly, we need the influxdb python library, which we can get using pip:\n\n\nCode\npip install influxdb\n\n\nNote that I’m not using the very similarly-named influxdb-client library- they have very different syntaxes!\nIn our python script, we can open a connection to this DB with the following:\n\n\nCode\nfrom influxdb import InfluxDBClient\n# Connect to our database\nhost = 'localhost'\nport = 8086\nusername = 'grafana'\npassword = 'grafana'\ndatabase = 'home_measurements'\n\nclient = InfluxDBClient(host=host,\n                        port=port,\n                        username=username,\n                        password=password,\n                        database=database)\n\n\nWrites to the database need to be in JSON format, and the following syntax seems to be standard. In my case, I have sensor_location_string='indoor' and sensor_location_description_string='living_room'. We’ll be able to group/subset our database by these variables later.\n\n\nCode\nmeasurements = [\n    {\n        \"measurement\": sensor_location_string,\n        \"tags\": {\n            \"location\": sensor_location_description_string},\n        \"time\": timestamp,\n        \"fields\": {\n            \"temperature\" : tempC,\n            \"pressure\": press_hPa}\n    }]\n\n# Write them to our database\nclient.write_points(measurements)\n\n\nMy entire script is here:\n\n\nCode\nfrom PiicoDev_TMP117 import PiicoDev_TMP117\nfrom PiicoDev_MS5637 import PiicoDev_MS5637\nfrom datetime import datetime\nfrom influxdb import InfluxDBClient\n\n# initialise the sensors\ntempSensor = PiicoDev_TMP117()\npressure = PiicoDev_MS5637()\n\n# Connect to our database\nhost = 'localhost'\nport = 8086\nusername = 'grafana'\npassword = 'grafana'\ndatabase = 'home_measurements'\n\nclient = InfluxDBClient(host=host,\n                        port=port,\n                        username=username,\n                        password=password,\n                        database=database)\n\n\n# Get our time measurements\nnow = datetime.now()\ntimestamp = datetime.utcnow()\ntimestamp_human = now.strftime(\"%d-%m-%YT%H:%M:%S\")\n\n# Measure the sensors\ntempC = tempSensor.readTempC()\npress_hPa = pressure.read_pressure()\n\n# Arrange in the correct JSON\nmeasurements = [\n    {\n        \"measurement\": \"indoor\",\n        \"tags\": {\n            \"location\": \"living_room\"},\n        \"time\": timestamp,\n        \"fields\": {\n            \"temperature\" : tempC,\n            \"pressure\": press_hPa}\n    }]\n\n# Write them to our database\nclient.write_points(measurements)\n\n\n\n\nThe script makes a single measurement. We can schedule it using the unix command-line utility cron. Run crontab -e and you should see the following:\n# Edit this file to introduce tasks to be run by cron.\n# \n# Each task to run has to be defined through a single line\n# indicating with different fields when the task will be run\n# and what command to run for the task\n# \n# To define the time you can provide concrete values for\n# minute (m), hour (h), day of month (dom), month (mon),\n# and day of week (dow) or use '*' in these fields (for 'any').\n# \n# Notice that tasks will be started based on the cron's system\n# daemon's notion of time and timezones.\n# \n# Output of the crontab jobs (including errors) is sent through\n# email to the user the crontab file belongs to (unless redirected).\n# \n# For example, you can run a backup of all your user accounts\n# at 5 a.m every week with:\n# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/\n# \n# For more information see the manual pages of crontab(5) and cron(8)\n# \n# m h  dom mon dow   command\n\nAdd the following write at the end:\n*/5 * * * * /usr/bin/python /path/to/your/script/script.py &\nThis tells our Pi to execute the command /usr/bin/python /path/to/your/script/script.py & at intervals of 5 minutes. Give it a go!\n\n\n\n\nNext we’ll install grafana, an open source dashboard service. To get it on the Pi, we need to add the Grafana key used to authenticate packages, add the repository to the source list, update the package list and then finally install grafana-rpi 1\n\n\nCode\nwget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\necho \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\nsudo apt-get update\nsudo apt-get install -y grafana-rpi\n\n\nWe then need to start the Grafana instance running, and also make sure that it starts whenever the Pi boots up.\n\n\nCode\nsudo systemctl unmask grafana-server.service\nsudo systemctl start grafana-server\nsudo systemctl enable grafana-server.service\n\n\nNearly there! We just need to point our Grafana instance to our database, which we do using the GUI."
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html#setting-up-the-pi",
    "href": "posts/raspberry-pi-data-logging/index.html#setting-up-the-pi",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "",
    "text": "I’m using a Raspberry Pi Zero and the PiicoDev precision temperature and pressure sensors, purchased from Core Electronics. They were incredibly easy to set up- just plug and play into a PiicoDev adaptor attached to the Pi’s header pins. The Pi itself was a standard configuration, just Raspberry Pi OS (5.15.84+ “Bullseye”).\nNext, initialise the I2C interface in the Raspberry Pi configuration menu or by using\n\n\nCode\nsudo raspi-config nonint do_i2c 0\n\n\nat the command line. Note that the zero means “on” here! We next need the piicodev python package from pip:\n\n\nCode\npip install piicodev\n\n\nUsing the piicodev package, we can then grab values from these sensors in the following way:\n\n\nCode\n# Import the packages\nfrom PiicoDev_TMP117 import PiicoDev_TMP117 # temperature sensos\nfrom PiicoDev_MS5637 import PiicoDev_MS5637 # pressure sensor\n\n# initialise the sensors\ntempSensor = PiicoDev_TMP117()\npressure = PiicoDev_MS5637()\n\n# read the values\ntempC = tempSensor.readTempC()\npress_hPa = pressure.read_pressure()\n\n\nSo if you want to just show those measurements on the commandline, you can just wrap this in a while loop and add some print statements. I wanted to have things running in the background and the results being saved in a nice format, however, so we also need a database to write to and a way to plot the results."
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html#making-an-influx-database",
    "href": "posts/raspberry-pi-data-logging/index.html#making-an-influx-database",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "",
    "text": "Whilst we could write things to a csv file, having a proper database connection has a number of benefits (such as scaling better if/when you have multiple sensors in different locations, and interfacing very nicely with the dashboard service we’ll be using). InfluxDB is an open-source platform built for working with time-series data, so it’s a sensible choice.\nFirst, we install it on our Pi. Which version? The latest online is Version 2.7, but that’s only available for 64-bit architectures, which my Pi Zero sadly isn’t. As of April 2023, version 1.6.7 is available in the standard Raspbian channels, but I’m using version 1.8.1 here.\nTo get that, we need to add the Influx repository\n\n\nCode\ncurl https://repos.influxdata.com/influxdata-archive.key | gpg --dearmor | sudo tee /usr/share/keyrings/influxdb-archive-keyring.gpg &gt;/dev/null\n\n\nand then add that to our source list. Note that $(lsb_release -cs) gets our version of Raspberry Pi OS, which in my case is “Bullseye”.\n\n\nCode\necho \"deb [signed-by=/usr/share/keyrings/influxdb-archive-keyring.gpg] https://repos.influxdata.com/debian $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/influxdb.list\n\n\nWe can then just do:\n\n\nCode\nsudo apt update && sudo apt install influxdb\n\n\nIf everything goes well, that should all finish without any errors. We then need to start the influx service running, and make sure that it starts whenever our Pi boots up.\n\n\nCode\nsudo systemctl unmask influxdb\nsudo systemctl enable influxdb\nsudo systemctl start influxdb\n\n\nYou should now be able to run influx at the command line, which changes to the influx prompt. We need to create a database and make a new user, which we can do with the following commands. Here the new user will be called grafana, with grafana as the password.\n\n\nCode\ncreate database home_measurements\nuse home_measurements\n\ncreate user grafana with password 'grafana' with all privileges\ngrant all privileges on home to grafana"
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html#writing-to-our-database",
    "href": "posts/raspberry-pi-data-logging/index.html#writing-to-our-database",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "",
    "text": "We can write our measurments to our database with a short python script. Firstly, we need the influxdb python library, which we can get using pip:\n\n\nCode\npip install influxdb\n\n\nNote that I’m not using the very similarly-named influxdb-client library- they have very different syntaxes!\nIn our python script, we can open a connection to this DB with the following:\n\n\nCode\nfrom influxdb import InfluxDBClient\n# Connect to our database\nhost = 'localhost'\nport = 8086\nusername = 'grafana'\npassword = 'grafana'\ndatabase = 'home_measurements'\n\nclient = InfluxDBClient(host=host,\n                        port=port,\n                        username=username,\n                        password=password,\n                        database=database)\n\n\nWrites to the database need to be in JSON format, and the following syntax seems to be standard. In my case, I have sensor_location_string='indoor' and sensor_location_description_string='living_room'. We’ll be able to group/subset our database by these variables later.\n\n\nCode\nmeasurements = [\n    {\n        \"measurement\": sensor_location_string,\n        \"tags\": {\n            \"location\": sensor_location_description_string},\n        \"time\": timestamp,\n        \"fields\": {\n            \"temperature\" : tempC,\n            \"pressure\": press_hPa}\n    }]\n\n# Write them to our database\nclient.write_points(measurements)\n\n\nMy entire script is here:\n\n\nCode\nfrom PiicoDev_TMP117 import PiicoDev_TMP117\nfrom PiicoDev_MS5637 import PiicoDev_MS5637\nfrom datetime import datetime\nfrom influxdb import InfluxDBClient\n\n# initialise the sensors\ntempSensor = PiicoDev_TMP117()\npressure = PiicoDev_MS5637()\n\n# Connect to our database\nhost = 'localhost'\nport = 8086\nusername = 'grafana'\npassword = 'grafana'\ndatabase = 'home_measurements'\n\nclient = InfluxDBClient(host=host,\n                        port=port,\n                        username=username,\n                        password=password,\n                        database=database)\n\n\n# Get our time measurements\nnow = datetime.now()\ntimestamp = datetime.utcnow()\ntimestamp_human = now.strftime(\"%d-%m-%YT%H:%M:%S\")\n\n# Measure the sensors\ntempC = tempSensor.readTempC()\npress_hPa = pressure.read_pressure()\n\n# Arrange in the correct JSON\nmeasurements = [\n    {\n        \"measurement\": \"indoor\",\n        \"tags\": {\n            \"location\": \"living_room\"},\n        \"time\": timestamp,\n        \"fields\": {\n            \"temperature\" : tempC,\n            \"pressure\": press_hPa}\n    }]\n\n# Write them to our database\nclient.write_points(measurements)\n\n\n\n\nThe script makes a single measurement. We can schedule it using the unix command-line utility cron. Run crontab -e and you should see the following:\n# Edit this file to introduce tasks to be run by cron.\n# \n# Each task to run has to be defined through a single line\n# indicating with different fields when the task will be run\n# and what command to run for the task\n# \n# To define the time you can provide concrete values for\n# minute (m), hour (h), day of month (dom), month (mon),\n# and day of week (dow) or use '*' in these fields (for 'any').\n# \n# Notice that tasks will be started based on the cron's system\n# daemon's notion of time and timezones.\n# \n# Output of the crontab jobs (including errors) is sent through\n# email to the user the crontab file belongs to (unless redirected).\n# \n# For example, you can run a backup of all your user accounts\n# at 5 a.m every week with:\n# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/\n# \n# For more information see the manual pages of crontab(5) and cron(8)\n# \n# m h  dom mon dow   command\n\nAdd the following write at the end:\n*/5 * * * * /usr/bin/python /path/to/your/script/script.py &\nThis tells our Pi to execute the command /usr/bin/python /path/to/your/script/script.py & at intervals of 5 minutes. Give it a go!"
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html#displaying-the-results-with-grafana",
    "href": "posts/raspberry-pi-data-logging/index.html#displaying-the-results-with-grafana",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "",
    "text": "Next we’ll install grafana, an open source dashboard service. To get it on the Pi, we need to add the Grafana key used to authenticate packages, add the repository to the source list, update the package list and then finally install grafana-rpi 1\n\n\nCode\nwget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\necho \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list\nsudo apt-get update\nsudo apt-get install -y grafana-rpi\n\n\nWe then need to start the Grafana instance running, and also make sure that it starts whenever the Pi boots up.\n\n\nCode\nsudo systemctl unmask grafana-server.service\nsudo systemctl start grafana-server\nsudo systemctl enable grafana-server.service\n\n\nNearly there! We just need to point our Grafana instance to our database, which we do using the GUI."
  },
  {
    "objectID": "posts/raspberry-pi-data-logging/index.html#footnotes",
    "href": "posts/raspberry-pi-data-logging/index.html#footnotes",
    "title": "Dashboards and data logging with a Raspberry Pi",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that I needed to run sudo apt-get install -y grafana-rpi, and not sudo apt-get install -y grafana (as, for example, the Grafana docs suggest here). When I ran sudo apt-get install -y grafana, my grafana server refused to start, similary to this github issue.↩︎"
  },
  {
    "objectID": "posts/a-macqaurie-ML-masterclass/index.html",
    "href": "posts/a-macqaurie-ML-masterclass/index.html",
    "title": "A Macquarie University Machine Learning masterclass”",
    "section": "",
    "text": "I recently organised a Machine Learning masterclass for the Macquarie Astrophysics department. A collaborator of mine, Professor Benoit Liquet-Weiland from the statistics department, is writing a Machine Learning textbook (which you can see here) and very kindly agreed to present a two-afternoon course on supervised and unsupervised learning.\nThe first afternoon covered a wide range of things, starting from linear models and logistic regression, then moved on to regularisation and finished on softmax models (i.e. a shallow neural network) for multi-class classification problem.\nThe second day…\nIt was great to see lots of the statistics and ML tools I’m familiar with being explained from a statistics perspective.\n\nTo add\n\nDay 2\nReally nice classification task with non-linear inputs (end of day 1). Can also do the same thing with multiclass data, even when we have more labels than features.\nInteresting to compare the available R and python tools. Benoit had to work hard to get glmnet working properly in python!"
  },
  {
    "objectID": "posts/a-galaxy-lacking-dark-matter/index.html",
    "href": "posts/a-galaxy-lacking-dark-matter/index.html",
    "title": "A galaxy lacking dark matter?",
    "section": "",
    "text": "I wrote a short post about the galaxy NGC 1052–DF2, which was the subject of recent work suggesting that it didn’t have any dark matter. You can read it on medium, here"
  },
  {
    "objectID": "posts/neural-networks-in-python/index.html",
    "href": "posts/neural-networks-in-python/index.html",
    "title": "Neural Networks in Python",
    "section": "",
    "text": "I recently bought the excellent book Hand-On Machine Learning with SciKitLearn and TensorFlow and decided to write my own simple neural network in python and numpy. Another really clear tutorial I’ve found is the online book by Michael Nielson, available here. Lots of my code is based heavily on his example!"
  },
  {
    "objectID": "posts/neural-networks-in-python/index.html#neural-networks",
    "href": "posts/neural-networks-in-python/index.html#neural-networks",
    "title": "Neural Networks in Python",
    "section": "Neural Networks",
    "text": "Neural Networks\nSo what is a Neural Network? At their core, neural networks are simple a collection of ‘neurons’. These neurons are little blocks of linear algebra operators which take a number of inputs and give you back an output. If we take the simple case of just having one neuron, things look deceptively simple!\nSay you want to decide whether or not to go and play football in the park. A number of outside factors might influence this decision, but we’ll just look at two: “is the weather sunny?” and “is anyone around to go and play with you?”. These are both yes or no questions, but you might assign different importances to their answers. Perhaps it’s a lovely sunny day but you’d rather sit inside and watch TV. Or perhaps there no one is around to play with but you’re happy practising your free-kicks on your own anyway. You’d assign different weights to the inputs before making your decision.\nAnother factor to think about is how much you enjoy playing football. If you don’t like it very much at all, no matter if it’s sunny and there are loads of people playing, you’re probably still going to stay indoors. On the other hand, if you absolutely love football then you’re more likely to go and play in the pouring rain! We’d call this internal factor your bias for getting an ice-cream. .\nTo stretch this metaphor a bit far, we’re actually going to put numbers on these weights- so, say, we’ll give the question ‘is it sunny?’ a weight of \\(1.0\\) (because you’d go and play rain-or-shine) and ‘are your friends free?’ a weight of \\(3.0\\) (since you don’t like playing on your own). Your internal bias is \\(2.0\\).\nNow, an artificial neuron takes the two inputs (“Sunny?” and “Anyone else playing?”) and multiplies them by a numerical weights we’ve assigned. It then simply says ‘Is this number greater than the bias?’ and either outputs \\(1\\) (if it is) or \\(0\\) (if it isn’t). So, in our case, if the weather is good but your friends are busy, the neuron comes up with \\(1 \\times 1\\) (the good weather weight) \\(+ 0 \\times x 3\\) (the friends weight). Since this is less than your bias of \\(2\\), the neuron outputs \\(0\\) (and you don’t go and play football). If your friends were playing, but it was raining, the neuron would output \\(1\\) (since \\(0\\times1 +1\\times3 &gt;2\\)) and you would go and play.\nThe key to neural networks is to think about things the other way around. Instead of knowing the weights and biases to start with, we know the outcomes- for example, we could keep a log of the weather and number of other players every day for a year, as well as the number of times you played football. We’d then train the neuron on all of the data by fiddling with the values of the weights and biases until it was able to reproduce the historical outcomes with the inputs- and then use these ‘best guess’ weights and biases to predict whether or not you’re going to play football tomorrow!"
  },
  {
    "objectID": "posts/neural-networks-in-python/index.html#implementation",
    "href": "posts/neural-networks-in-python/index.html#implementation",
    "title": "Neural Networks in Python",
    "section": "Implementation",
    "text": "Implementation\nThings are obviously a bit more complicated than this in reality, of course. We can’t use a simple ‘step’ function for the neuron’s output (either \\(1\\) or \\(0\\)), because way we train a network requires that a tiny change we make to the weights and biases leads to tiny changes in the output. With a step function, it’s not possible to make a small change- it’s all or nothing! We instead use functions which behave a bit like the step function, but still have smooth behaviour- for example a sigmoid function or a a general class called ‘Rectified Linear Units’ (ReLUs).\nWhen we have many neurons acting together, the simple weights and bias numbers before turn into a weights matrix and a bias vector. We arrange these neurons in a number of layers, with the simplest case having each neuron in the first layer sending its outputs to each neuron in the second layer. An example of these connections, from Michael Nielson’s book, are shown below:\n\n\n\nAn example neuaral network"
  },
  {
    "objectID": "posts/neural-networks-in-python/index.html#digit-classification",
    "href": "posts/neural-networks-in-python/index.html#digit-classification",
    "title": "Neural Networks in Python",
    "section": "Digit Classification",
    "text": "Digit Classification\nA classic place to learn and practice with neural networks is the MNIST dataset. This is a collection of 60,000 hand written digits from 0-9, packaged as a 28 pixel by 28 pixel image and a label of the correct answer for that image. Michael Nielson talks you through building a neural network in python with \\(28\\times28=784\\) inputs (one per pixel), one ‘hidden’ layer of neurons to do the thinking and 10 outputs (which will be the probability that the input digit is a 0, 1, 2, etc.)\nThe key to the whole process is the ‘backpropogation’ algorithm, which takes the difference between the network’s prediction and the actual label, then propagates these errors backwards in order to adjust the weights and biases. His code uses a neat for loop for this, but I couldn’t quite get my head around it until I’d written it out simply and without the loop.\nWhen I tested it, the neural network gets the classification of an unseen set of handwritten digits correct 95% of the time! I tried it out myself by sketching a quick number 9 in paint, then downscaling the image to be 28x28 pixels and running it through the code- and it was correct! Even though I’ve written the code myself, it still feels a bit like black magic…\nThe network takes a while to run through all of the training data (around 10 minutes), but once you’ve worked out the best weights and biases, classifying a new digit takes no time at all. The whole code is available on github, here"
  },
  {
    "objectID": "posts/neural-networks-in-python/index.html#tensorflow",
    "href": "posts/neural-networks-in-python/index.html#tensorflow",
    "title": "Neural Networks in Python",
    "section": "TensorFlow",
    "text": "TensorFlow\nOf course, hard coded for loops aren’t great for extending this network to more layers, or choosing a different optimisation method. For doing real machine learning problems, you’ll need something like TensorFlow. I wrote a similar network using the tensorflow tools (with more neurons and some extra bells and whistles), which you can also find on github- this one gets to around 98% successful classification rate.\nTensorFlow also has some very nice options for analysing your network’s performance. Here is a screenshot of the accuracy of my network, with each run corresponding to different tweaks of the various hyperparameters (e.g the type of optimizer, the dropout probability, etc).\n\n\n\nTensorflow training accuracy\n\n\nThe green run uses the ‘Adam’ optimiser, but I’d set the learning rate at too large a value- it’s interesting how these choices can make a good 3-4% difference on the outcomes!\nThe best networks can reach \\(&gt;99%\\) accuracy (such as, for example, this one). They do this using a number of different techniques, such as augmenting the training data (by e.g rotating or zooming the numbers slightly, to build a larger training set) and/or applying ‘convolutional neural networks’ which can apply learnable filters to the images beforehand."
  },
  {
    "objectID": "old/Pure_blog/LICENSE.html",
    "href": "old/Pure_blog/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nSoftware License Agreement (BSD License)\nCopyright 2013 Yahoo! Inc. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n* Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution.\n\n* Neither the name of the Yahoo! Inc. nor the\n  names of its contributors may be used to endorse or promote products\n  derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL YAHOO! INC. BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  }
]